{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3xQa6VlNAOe"
   },
   "source": [
    "# Effective Missing Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0mG3n0qRNTSA",
    "outputId": "cb0eadbd-b8a8-41b1-e3b4-7f44b47a8804"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load MovieLens 100K dataset into a dataframe of pandas\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "hFJT0QWudp1P"
   },
   "outputs": [],
   "source": [
    "# Select 500 most active users and 500 most active items from the dataset\n",
    "n_most_active_users = 500\n",
    "n_most_active_items = 500\n",
    "\n",
    "user_ids = df.groupby('user_id').count().sort_values(by='rating', ascending=False).head(n_most_active_users).index\n",
    "item_ids = df.groupby('item_id').count().sort_values(by='rating', ascending=False).head(n_most_active_items).index\n",
    "df = df[(df['user_id'].isin(user_ids)) & (df['item_id'].isin(item_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "r012fu0jJkJc"
   },
   "outputs": [],
   "source": [
    "# Map new internal ID for items\n",
    "i_ids = df['item_id'].unique().tolist()\n",
    "item_dict = dict(zip(i_ids, [i for i in range(len(i_ids))]))\n",
    "df['item_id'] = df['item_id'].map(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>881171488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>253</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "1      186        0       3  891717742\n",
       "3      244        1       2  880606923\n",
       "5      298        2       4  884182806\n",
       "6      115        3       2  881171488\n",
       "7      253        4       5  891628467"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ3rlC7jO6WJ"
   },
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwS00lvHO-ca",
    "outputId": "f9f3a031-b4e7-439e-c3e3-c165b2286d19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-27058b8d4712>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
      "<ipython-input-65-27058b8d4712>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n"
     ]
    }
   ],
   "source": [
    "# The number of training users and active users\n",
    "n_training_users = 300\n",
    "n_active_users = n_most_active_users - n_training_users\n",
    "\n",
    "# The number of GIVEN ratings for active users\n",
    "GIVEN = 20\n",
    "\n",
    "# Randomly select users from the most active users as training set\n",
    "random_uids = np.random.choice(df.user_id.unique(), n_training_users, replace=False)\n",
    "train_df = df[df['user_id'].isin(random_uids)]\n",
    "# Map new internal ID for all users in the training set\n",
    "u_ids = train_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "train_df['user_id'] = train_df['user_id'].map(user_dict)\n",
    "\n",
    "# The rest of users are active users for testing\n",
    "remain_df = df[~df['user_id'].isin(random_uids)]\n",
    "# Map new internal ID for all active users\n",
    "u_ids = remain_df['user_id'].unique().tolist()\n",
    "user_dict = dict(zip(u_ids, [i for i in range(len(u_ids))]))\n",
    "remain_df['user_id'] = remain_df['user_id'].map(user_dict)\n",
    "\n",
    "# Randomly select GIVEN ratings for active users\n",
    "# n=20 means 20 items should be returmed from each group\n",
    "active_df = remain_df.groupby('user_id').sample(n=20, random_state=1024)\n",
    "\n",
    "test_df = remain_df[~remain_df.index.isin(active_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-ke62G3jiYb",
    "outputId": "1c135984-edb4-4f2b-fc73-9225d86a0c38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       " user_id                                                    ...                  \n",
       " 0        0.0  2.0  0.0  4.0  0.0  4.0  4.0  0.0  0.0  2.0  ...  0.0  4.0  4.0   \n",
       " 1        0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n",
       " 2        4.0  0.0  5.0  0.0  1.0  0.0  3.0  2.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 3        3.0  0.0  4.0  0.0  0.0  3.0  2.0  2.0  0.0  5.0  ...  0.0  4.0  0.0   \n",
       " 4        0.0  4.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  4.0  ...  0.0  0.0  3.0   \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 295      0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  3.0  5.0  ...  0.0  0.0  0.0   \n",
       " 296      0.0  0.0  5.0  4.0  0.0  1.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0   \n",
       " 297      0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 298      0.0  0.0  0.0  3.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  5.0   \n",
       " 299      0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  3.0  0.0  ...  4.0  0.0  0.0   \n",
       " \n",
       " item_id  493  494  495  496  497  498  499  \n",
       " user_id                                     \n",
       " 0        0.0  3.0  3.0  0.0  0.0  0.0  0.0  \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2        4.0  1.0  0.0  0.0  0.0  0.0  2.0  \n",
       " 3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  \n",
       " 295      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 296      0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       " 297      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 298      0.0  0.0  3.0  0.0  0.0  0.0  0.0  \n",
       " 299      5.0  0.0  0.0  3.0  0.0  0.0  0.0  \n",
       " \n",
       " [300 rows x 500 columns],\n",
       " item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       " user_id                                                    ...                  \n",
       " 0        3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 3        0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  4.0  0.0   \n",
       " \n",
       " item_id  493  494  495  496  497  498  499  \n",
       " user_id                                     \n",
       " 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 4        0.0  0.0  0.0  3.0  0.0  0.0  0.0  \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  \n",
       " 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 197      0.0  4.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " \n",
       " [200 rows x 500 columns],\n",
       " item_id  0    1    2    3    4    5    6    7    8    9    ...  490  491  492  \\\n",
       " user_id                                                    ...                  \n",
       " 0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 1        0.0  0.0  4.0  4.0  4.0  0.0  0.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 2        4.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 3        4.0  0.0  5.0  0.0  0.0  0.0  4.0  2.0  0.0  2.0  ...  0.0  2.0  0.0   \n",
       " 4        0.0  0.0  0.0  0.0  5.0  5.0  0.0  3.0  5.0  0.0  ...  0.0  4.0  0.0   \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       " 195      0.0  0.0  5.0  0.0  0.0  0.0  4.0  0.0  0.0  4.0  ...  0.0  0.0  0.0   \n",
       " 196      4.0  0.0  4.0  0.0  0.0  4.0  3.0  4.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 197      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  5.0  ...  0.0  0.0  4.0   \n",
       " 198      4.0  0.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       " 199      0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  3.0  0.0  0.0   \n",
       " \n",
       " item_id  493  494  495  496  497  498  499  \n",
       " user_id                                     \n",
       " 0        0.0  0.0  0.0  4.0  0.0  3.0  0.0  \n",
       " 1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 3        0.0  0.0  0.0  0.0  4.0  0.0  0.0  \n",
       " 4        0.0  0.0  0.0  0.0  0.0  3.0  0.0  \n",
       " ...      ...  ...  ...  ...  ...  ...  ...  \n",
       " 195      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 196      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 197      0.0  0.0  2.0  3.0  0.0  0.0  0.0  \n",
       " 198      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " 199      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       " \n",
       " [200 rows x 500 columns])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the format of datasets to matrices\n",
    "df_zeros = pd.DataFrame({'user_id': np.tile(np.arange(0, n_training_users), n_most_active_items), 'item_id': np.repeat(np.arange(0, n_most_active_items), n_training_users), 'rating': 0})\n",
    "train_ds = df_zeros.merge(train_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "\n",
    "df_zeros = pd.DataFrame({'user_id': np.tile(np.arange(0, n_active_users), n_most_active_items), 'item_id': np.repeat(np.arange(0, n_most_active_items), n_active_users), 'rating': 0})\n",
    "active_ds = df_zeros.merge(active_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "test_ds = df_zeros.merge(test_df, how='left', on=['user_id', 'item_id']).fillna(0.).pivot_table(values='rating_y', index='user_id', columns='item_id')\n",
    "\n",
    "train_ds, active_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "yElYv2TDKKGu"
   },
   "outputs": [],
   "source": [
    "# Predicting All Missing Data in training set\n",
    "imputed_train_ds = train_ds.values.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy4FurbHD4dt"
   },
   "source": [
    "# Missing Value Prediction Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "zYh1bVd0ncz3"
   },
   "outputs": [],
   "source": [
    "## The following parameters are required in the given report, \n",
    "## which is named \"Effective Missing Data Prediction for Collaborative Filtering\", \n",
    "## and we need to use them. We are not changing the values for this implementation.\n",
    "LAMBDA = 0.7    # λ\n",
    "GAMMA = 10      # γ\n",
    "DELTA = 10      # δ\n",
    "ITA = 0.7       # η\n",
    "THETA = 0.7     # θ\n",
    "EPSILON = 1e-9\n",
    "\n",
    "# For the prediction of the missing values we require the user by user and item by item pearson similarity matrix\n",
    "# as per the solution in the paper. So here first the user by user pearson similarity matrix is calculated followed\n",
    "# by item by item pearson similarity matrix and at last using these data the missing data has been predicted.\n",
    "\n",
    "\n",
    "# Pearson similarity calculation for each pair of users in the training data set.\n",
    "# Initialising the user by user pearson similarity matrix for the training data set.\n",
    "np_user_pearson_corr_user_by_user = np.zeros((n_training_users, n_training_users))\n",
    "imputed_train_ds = pd.DataFrame(imputed_train_ds)\n",
    "\n",
    "# Looping the users against each other for forming the pearson similarity matrix.\n",
    "for selected_user_data in imputed_train_ds.itertuples():\n",
    "    for looping_user_data in imputed_train_ds.itertuples():\n",
    "        \n",
    "        # Forming the ratings vector for the pair of users.\n",
    "        looping_user_vec = np.array(looping_user_data[1:])\n",
    "        selected_user_vec = np.array(selected_user_data[1:])\n",
    "        \n",
    "        # Finding the indices of items commonly rated by the users.\n",
    "        common_indices = np.intersect1d(np.where(selected_user_vec > 0), np.where(looping_user_vec>0))\n",
    "        # If there are no commonly rated items then the user by user similarity is considered 0 and will proceed with the next pair of users. The below condition will make it possible.\n",
    "        if not len(common_indices):\n",
    "            continue\n",
    "        \n",
    "        # Finding the average ratings of the users.\n",
    "        looping_user_vec_avg = np.sum(looping_user_vec)/(np.count_nonzero(looping_user_vec) + EPSILON)\n",
    "        selected_user_vec_avg = np.sum(selected_user_vec)/(np.count_nonzero(selected_user_vec) + EPSILON)\n",
    "        \n",
    "        # Centering the ratings of the users by subtracting the average ratings from ratings.\n",
    "        looping_user_vec_common_avgs= looping_user_vec[common_indices] - looping_user_vec_avg\n",
    "        selected_user_vec_common_avgs = selected_user_vec[common_indices] - selected_user_vec_avg\n",
    "        \n",
    "        # Calculation of squares of the centered user ratings.\n",
    "        looping_user_vec_common_squares = np.square(looping_user_vec_common_avgs)\n",
    "        selected_user_vec_common_squares = np.square(selected_user_vec_common_avgs)\n",
    "        \n",
    "        # Similarity calculation between the users.\n",
    "        similarity = (np.sum(looping_user_vec_common_avgs * selected_user_vec_common_avgs)/ \n",
    "                      (np.sqrt(np.sum(looping_user_vec_common_squares)) * np.sqrt(np.sum(selected_user_vec_common_squares)) +\n",
    "                       EPSILON))\n",
    "        \n",
    "        # Applying the significance weighting to calculated similarities\n",
    "        weighted_similarity = (min(len(common_indices), GAMMA)/ GAMMA) * similarity\n",
    "        \n",
    "        # Adding the weighted similarity to the pearson similarity matrix\n",
    "        np_user_pearson_corr_user_by_user[selected_user_data[0]][looping_user_data[0]] = weighted_similarity\n",
    "    \n",
    "# Pearson similarity calculation for each pair of items in the training data set.\n",
    "# Initialising the item by item pearson similarity matrix for the training data set.\n",
    "np_item_pearson_corr_item_by_item = np.zeros((n_most_active_items, n_most_active_items))\n",
    "\n",
    "# Looping the items against each other for forming the pearson similarity matrix.\n",
    "for selected_item_data in imputed_train_ds.transpose().itertuples():\n",
    "    for looping_item_data in imputed_train_ds.transpose().itertuples():\n",
    "        \n",
    "        # Forming the ratings vector for the pair of items.\n",
    "        looping_item_vec = np.array(looping_item_data[1:])\n",
    "        selected_item_vec = np.array(selected_item_data[1:])\n",
    "        \n",
    "        # Finding the indices of users who have rated the pair of items.\n",
    "        common_indices = np.intersect1d(np.where(selected_item_vec > 0), np.where(looping_item_vec>0))\n",
    "        # If there are no common users then the item by item similarity is considered 0 and will proceed with the next pair of items. The below condition will make it possible.\n",
    "        if not len(common_indices):\n",
    "            continue\n",
    "        \n",
    "        # Finding the average ratings of the items.\n",
    "        looping_item_vec_avg = np.sum(looping_item_vec)/(np.count_nonzero(looping_item_vec) + EPSILON)\n",
    "        selected_item_vec_avg = np.sum(selected_item_vec)/(np.count_nonzero(selected_item_vec) + EPSILON)\n",
    "        \n",
    "        # Centering the ratings of the items by subtracting the average ratings from ratings.\n",
    "        looping_item_vec_common_avgs= looping_item_vec[common_indices] - looping_item_vec_avg\n",
    "        selected_item_vec_common_avgs = selected_item_vec[common_indices] - selected_item_vec_avg\n",
    "        \n",
    "        # Calculation of squares of the centered item ratings.\n",
    "        looping_item_vec_common_squares = np.square(looping_item_vec_common_avgs)\n",
    "        selected_item_vec_common_squares = np.square(selected_item_vec_common_avgs)\n",
    "        \n",
    "        # Similarity calculation between the items.\n",
    "        similarity = (np.sum(looping_item_vec_common_avgs * selected_item_vec_common_avgs)/ \n",
    "                      (np.sqrt(np.sum(looping_item_vec_common_squares)) * np.sqrt(np.sum(selected_item_vec_common_squares)) +\n",
    "                       EPSILON))\n",
    "        \n",
    "        # Applying the significance weighting to calculated similarities\n",
    "        weighted_similarity = (min(len(common_indices), DELTA)/ DELTA) * similarity\n",
    "        \n",
    "        # Adding the weighted similarity to the pearson similarity matrix\n",
    "        np_item_pearson_corr_item_by_item[selected_item_data[0]][looping_item_data[0]] = weighted_similarity\n",
    "\n",
    "# Missing value predictions\n",
    "# Looping thorugh each user, item , rating combination\n",
    "for (current_user, current_item), rating in np.ndenumerate(imputed_train_ds.values):\n",
    "    \n",
    "    # Condition to check whether the rating is 0. (Those ratings need to be predicted)\n",
    "    if not rating:\n",
    "        \n",
    "        # Finding similar user ids and item ids based on the condition mentioned in the paper.\n",
    "        similar_users_ids_with_current_user_condition_based = np.argwhere(np_user_pearson_corr_user_by_user[current_user] > ITA).flatten()\n",
    "        similar_items_ids_with_current_item_condition_based = np.argwhere(np_item_pearson_corr_item_by_item[current_item] > THETA).flatten()\n",
    "        \n",
    "        # removing the current user and current item from the array.\n",
    "        similar_users_ids_with_current_user_condition_based = similar_users_ids_with_current_user_condition_based[similar_users_ids_with_current_user_condition_based != current_user]\n",
    "        similar_items_ids_with_current_item_condition_based = similar_items_ids_with_current_item_condition_based[similar_items_ids_with_current_item_condition_based != current_item]\n",
    "        \n",
    "        # Skipping the calculation if there are no similar items and similar users.\n",
    "        if not len(similar_users_ids_with_current_user_condition_based) and not len(similar_items_ids_with_current_item_condition_based):\n",
    "            continue\n",
    "            \n",
    "        # Finding the pearson coefficients for similar users and items.    \n",
    "        pearson_coeff_similar_users = np_user_pearson_corr_user_by_user[current_user][similar_users_ids_with_current_user_condition_based]\n",
    "        pearson_coeff_similar_items = np_item_pearson_corr_item_by_item[current_item][similar_items_ids_with_current_item_condition_based]\n",
    "            \n",
    "        # Finding the similar users and items.\n",
    "        similar_users = imputed_train_ds.values[similar_users_ids_with_current_user_condition_based]\n",
    "        similar_items = imputed_train_ds.transpose().values[similar_items_ids_with_current_item_condition_based]\n",
    "        \n",
    "        # Calculating the current user and item ratings mean\n",
    "        current_user_mean = np.sum(imputed_train_ds.values[current_user]) / (np.count_nonzero(imputed_train_ds.values[current_user]) + EPSILON)\n",
    "        current_item_mean = np.sum(imputed_train_ds.transpose().values[current_item]) / (np.count_nonzero(imputed_train_ds.transpose().values[current_item]) + EPSILON)\n",
    "        \n",
    "        # Calculating the means of all the similar users and items.\n",
    "        similar_users_mean = np.sum(similar_users, axis=1)/ (np.count_nonzero(similar_users, axis=1) + EPSILON)\n",
    "        similar_items_mean = np.sum(similar_items, axis=1)/ (np.count_nonzero(similar_items, axis=1) + EPSILON)\n",
    "        \n",
    "        # Condition for finding the users from similar users who has rated current item.\n",
    "        mask_for_users = similar_users[:,current_item] > 0\n",
    "        # Condition for finding items from similar items which has been rated by the current user.\n",
    "        mask_for_items = similar_items[:,current_user] > 0\n",
    "        \n",
    "        # Calculation of the numerator values (pearson coeff * (similar user/item - similar user/item mean)) for both user and item based equations.\n",
    "        equation_numerator_for_users = pearson_coeff_similar_users[mask_for_users] * (similar_users[mask_for_users, current_item] - similar_users_mean[mask_for_users])\n",
    "        equation_numerator_for_items = pearson_coeff_similar_items[mask_for_items] * (similar_items[mask_for_items, current_user] - similar_items_mean[mask_for_items])\n",
    "        \n",
    "        # Calculation of the contribution by the similar users/items for determining the missing value for rating.\n",
    "        similar_user_contribution_for_missing_value = LAMBDA * (current_user_mean + np.sum(equation_numerator_for_users)/(np.sum(pearson_coeff_similar_users[mask_for_users]) + EPSILON))\n",
    "        similar_item_contribution_for_missing_value = (1 - LAMBDA) * (current_item_mean + np.sum(equation_numerator_for_items)/(np.sum(pearson_coeff_similar_items[mask_for_items]) + EPSILON))\n",
    "        \n",
    "        # Predicted missing value is fitted into the data set to use it for further calculations.\n",
    "        imputed_train_ds.loc[current_user,current_item] = similar_item_contribution_for_missing_value + similar_user_contribution_for_missing_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbOfVWTV_Aij"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txs8YjwTzuSP"
   },
   "source": [
    "### Compute Pearson Correlation Coefficient of All Pairs of Items between active set and imputed training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "KoOgX_axKKGw",
    "outputId": "37a7adbd-e0d6-4375-e28c-3940977896f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.773384</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.858157</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.631905</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.671784</td>\n",
       "      <td>3.329546</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.437049</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.702078</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.315039</td>\n",
       "      <td>3.621273</td>\n",
       "      <td>3.541033</td>\n",
       "      <td>3.377337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.027060</td>\n",
       "      <td>3.822324</td>\n",
       "      <td>4.064443</td>\n",
       "      <td>3.929967</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.793034</td>\n",
       "      <td>4.293254</td>\n",
       "      <td>3.880195</td>\n",
       "      <td>3.704583</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.679636</td>\n",
       "      <td>3.801295</td>\n",
       "      <td>3.558522</td>\n",
       "      <td>3.792968</td>\n",
       "      <td>3.723858</td>\n",
       "      <td>3.661998</td>\n",
       "      <td>3.535933</td>\n",
       "      <td>3.824936</td>\n",
       "      <td>3.744714</td>\n",
       "      <td>3.704883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.581206</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.687211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.895212</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.540864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.480987</td>\n",
       "      <td>3.553548</td>\n",
       "      <td>3.623580</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.243941</td>\n",
       "      <td>3.259192</td>\n",
       "      <td>3.619155</td>\n",
       "      <td>3.486359</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.423400</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.540970</td>\n",
       "      <td>3.677548</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.300217</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.139294</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.484309</td>\n",
       "      <td>3.415390</td>\n",
       "      <td>3.265096</td>\n",
       "      <td>3.171145</td>\n",
       "      <td>3.101744</td>\n",
       "      <td>3.191237</td>\n",
       "      <td>3.271225</td>\n",
       "      <td>3.254461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.450534</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.490534</td>\n",
       "      <td>3.357695</td>\n",
       "      <td>2.789510</td>\n",
       "      <td>3.578672</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.665430</td>\n",
       "      <td>3.187632</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.196297</td>\n",
       "      <td>2.999485</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.175066</td>\n",
       "      <td>3.047991</td>\n",
       "      <td>3.026877</td>\n",
       "      <td>3.054435</td>\n",
       "      <td>3.134980</td>\n",
       "      <td>3.025812</td>\n",
       "      <td>3.140561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4.068902</td>\n",
       "      <td>3.944301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.834134</td>\n",
       "      <td>3.943636</td>\n",
       "      <td>3.737538</td>\n",
       "      <td>3.849516</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.793275</td>\n",
       "      <td>3.817800</td>\n",
       "      <td>3.837375</td>\n",
       "      <td>3.799350</td>\n",
       "      <td>3.798869</td>\n",
       "      <td>3.597973</td>\n",
       "      <td>3.701528</td>\n",
       "      <td>3.811268</td>\n",
       "      <td>3.829540</td>\n",
       "      <td>3.876241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>3.011273</td>\n",
       "      <td>2.260679</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.132104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.247527</td>\n",
       "      <td>2.910556</td>\n",
       "      <td>2.742530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.782109</td>\n",
       "      <td>2.676711</td>\n",
       "      <td>3.069327</td>\n",
       "      <td>2.933046</td>\n",
       "      <td>2.771768</td>\n",
       "      <td>2.402720</td>\n",
       "      <td>2.496856</td>\n",
       "      <td>2.789270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.717306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>4.917099</td>\n",
       "      <td>3.989189</td>\n",
       "      <td>4.929022</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.229015</td>\n",
       "      <td>3.986037</td>\n",
       "      <td>4.272382</td>\n",
       "      <td>4.451433</td>\n",
       "      <td>3.572355</td>\n",
       "      <td>4.291366</td>\n",
       "      <td>...</td>\n",
       "      <td>3.207213</td>\n",
       "      <td>3.993434</td>\n",
       "      <td>4.684550</td>\n",
       "      <td>4.973383</td>\n",
       "      <td>3.809335</td>\n",
       "      <td>3.598022</td>\n",
       "      <td>2.979038</td>\n",
       "      <td>4.358734</td>\n",
       "      <td>3.801924</td>\n",
       "      <td>3.941878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>4.160124</td>\n",
       "      <td>2.802849</td>\n",
       "      <td>3.443326</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.169072</td>\n",
       "      <td>3.117646</td>\n",
       "      <td>3.253825</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.380384</td>\n",
       "      <td>3.377221</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886821</td>\n",
       "      <td>3.363340</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.362280</td>\n",
       "      <td>3.105172</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.963161</td>\n",
       "      <td>3.043053</td>\n",
       "      <td>3.178635</td>\n",
       "      <td>3.080179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>3.937146</td>\n",
       "      <td>3.675229</td>\n",
       "      <td>4.312272</td>\n",
       "      <td>3.805504</td>\n",
       "      <td>3.784405</td>\n",
       "      <td>3.559306</td>\n",
       "      <td>4.096597</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.360286</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.598918</td>\n",
       "      <td>3.796438</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.867507</td>\n",
       "      <td>3.555468</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.498796</td>\n",
       "      <td>3.639401</td>\n",
       "      <td>3.677522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    4.773384  2.000000  3.858157  4.000000  3.631905  4.000000  4.000000   \n",
       "1    4.027060  3.822324  4.064443  3.929967  5.000000  3.793034  4.293254   \n",
       "2    4.000000  3.581206  5.000000  3.687211  1.000000  3.895212  3.000000   \n",
       "3    3.000000  3.423400  4.000000  3.540970  3.677548  3.000000  2.000000   \n",
       "4    3.450534  4.000000  3.490534  3.357695  2.789510  3.578672  3.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "295  4.068902  3.944301  0.000000  3.834134  3.943636  3.737538  3.849516   \n",
       "296  3.011273  2.260679  5.000000  4.000000  3.132104  1.000000  3.247527   \n",
       "297  4.917099  3.989189  4.929022  5.000000  4.229015  3.986037  4.272382   \n",
       "298  4.160124  2.802849  3.443326  3.000000  3.169072  3.117646  3.253825   \n",
       "299  3.937146  3.675229  4.312272  3.805504  3.784405  3.559306  4.096597   \n",
       "\n",
       "          7         8         9    ...       490       491       492  \\\n",
       "0    3.671784  3.329546  2.000000  ...  3.437049  4.000000  4.000000   \n",
       "1    3.880195  3.704583  4.000000  ...  3.679636  3.801295  3.558522   \n",
       "2    2.000000  3.540864  0.000000  ...  3.480987  3.553548  3.623580   \n",
       "3    2.000000  3.300217  5.000000  ...  3.139294  4.000000  3.484309   \n",
       "4    3.665430  3.187632  4.000000  ...  3.196297  2.999485  3.000000   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "295  5.000000  3.000000  5.000000  ...  3.793275  3.817800  3.837375   \n",
       "296  2.910556  2.742530  1.000000  ...  1.782109  2.676711  3.069327   \n",
       "297  4.451433  3.572355  4.291366  ...  3.207213  3.993434  4.684550   \n",
       "298  4.000000  2.380384  3.377221  ...  1.886821  3.363340  5.000000   \n",
       "299  3.000000  3.000000  3.360286  ...  4.000000  3.598918  3.796438   \n",
       "\n",
       "          493       494       495       496       497       498       499  \n",
       "0    3.702078  3.000000  3.000000  3.315039  3.621273  3.541033  3.377337  \n",
       "1    3.792968  3.723858  3.661998  3.535933  3.824936  3.744714  3.704883  \n",
       "2    4.000000  1.000000  3.243941  3.259192  3.619155  3.486359  2.000000  \n",
       "3    3.415390  3.265096  3.171145  3.101744  3.191237  3.271225  3.254461  \n",
       "4    3.175066  3.047991  3.026877  3.054435  3.134980  3.025812  3.140561  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "295  3.799350  3.798869  3.597973  3.701528  3.811268  3.829540  3.876241  \n",
       "296  2.933046  2.771768  2.402720  2.496856  2.789270  1.000000  2.717306  \n",
       "297  4.973383  3.809335  3.598022  2.979038  4.358734  3.801924  3.941878  \n",
       "298  3.362280  3.105172  3.000000  2.963161  3.043053  3.178635  3.080179  \n",
       "299  5.000000  3.867507  3.555468  3.000000  3.498796  3.639401  3.677522  \n",
       "\n",
       "[300 rows x 500 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_train_ds = pd.DataFrame(imputed_train_ds)\n",
    "imputed_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq0uq1aHzu11",
    "outputId": "f6214a46-d63e-4fdc-e7dd-ccaed23b237d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01229161,  0.04031051,  0.00280008, ..., -0.01334131,\n",
       "         0.31727715, -0.20961611],\n",
       "       [-0.24496546,  0.2818903 ,  0.17869313, ...,  0.55986397,\n",
       "         0.36594731,  0.33537382],\n",
       "       [-0.42084656, -0.01262957,  0.27249925, ...,  0.27956129,\n",
       "         0.05013149,  0.12308677],\n",
       "       ...,\n",
       "       [ 0.22763596,  0.10310597, -0.02987605, ...,  0.38189949,\n",
       "         0.11947157,  0.0763004 ],\n",
       "       [ 0.05668059,  0.5216229 , -0.10993777, ...,  0.31617461,\n",
       "        -0.08481742,  0.08925924],\n",
       "       [ 0.14947792,  0.29389936, -0.01882839, ...,  0.13568678,\n",
       "         0.13235258, -0.18547186]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_user_pearson_corr = np.zeros((active_ds.shape[0], train_ds.shape[0]))\n",
    "\n",
    "# Compute Pearson Correlation Coefficient of All Pairs of Users between active set and imputed training set\n",
    "for i, user_i_vec in enumerate(active_ds.values):\n",
    "    for j, user_j_vec in enumerate(imputed_train_ds.values):\n",
    "        \n",
    "        # ratings corated by the current pair od users\n",
    "        mask_i = user_i_vec > 0\n",
    "        mask_j = user_j_vec > 0\n",
    "\n",
    "        # corrated item index, skip if there are no corrated ratings\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            continue\n",
    "\n",
    "        # average value of user_i_vec and user_j_vec\n",
    "        mean_user_i = np.sum(user_i_vec) / (np.sum(np.clip(user_i_vec, 0, 1)) + EPSILON)\n",
    "        mean_user_j = np.sum(user_j_vec) / (np.sum(np.clip(user_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "        # compute pearson corr\n",
    "        user_i_sub_mean = user_i_vec[corrated_index] - mean_user_i\n",
    "        user_j_sub_mean = user_j_vec[corrated_index] - mean_user_j\n",
    "\n",
    "        r_ui_sub_r_i_sq = np.square(user_i_sub_mean)\n",
    "        r_uj_sub_r_j_sq = np.square(user_j_sub_mean)\n",
    "\n",
    "        r_ui_sum_sqrt = np.sqrt(np.sum(r_ui_sub_r_i_sq))\n",
    "        r_uj_sum_sqrt = np.sqrt(np.sum(r_uj_sub_r_j_sq))\n",
    "\n",
    "        sim = np.sum(user_i_sub_mean * user_j_sub_mean) / (r_ui_sum_sqrt * r_uj_sum_sqrt + EPSILON)\n",
    "\n",
    "        # significance weighting\n",
    "        weighted_sim = (min(len(corrated_index), GAMMA) / GAMMA) * sim\n",
    "\n",
    "        active_user_pearson_corr[i][j] = weighted_sim\n",
    "\n",
    "active_user_pearson_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewTnN9kNb8Ys"
   },
   "source": [
    "## Predict Ratings of Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4ERndYXb8Ys",
    "outputId": "12607670-af61-403a-e4ce-f5e874bf8386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 3.33513081,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 4.43778735, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [4.20219527, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [3.72373725, 0.        , 3.77960363, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 4.02021054, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "test_ds_pred = np.zeros_like(test_ds.values)\n",
    "\n",
    "for (i, j), rating in np.ndenumerate(test_ds.values):\n",
    "\n",
    "    if rating > 0:\n",
    "\n",
    "        sim_user_ids = np.argsort(active_user_pearson_corr[i])[-1:-(K + 1):-1]\n",
    "\n",
    "        #==================user-based==================#\n",
    "        # the coefficient values of similar users\n",
    "        sim_val = active_user_pearson_corr[i][sim_user_ids]\n",
    "\n",
    "        # the average value of the current user's ratings\n",
    "        sim_users = imputed_train_ds.values[sim_user_ids]\n",
    "        user_mean = np.sum(active_ds.values[i]) / (np.sum(np.clip(active_ds.values[i], 0, 1)) + EPSILON)\n",
    "        sim_user_mean = np.sum(sim_users, axis=1) / (np.sum(np.clip(sim_users, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "        # select the users who rated item j\n",
    "        mask_rated_j = sim_users[:, j] > 0\n",
    "        \n",
    "        # sim(u, v) * (r_vj - mean_v)\n",
    "        sim_r_sum_mean = sim_val[mask_rated_j] * (sim_users[mask_rated_j, j] - sim_user_mean[mask_rated_j])\n",
    "        \n",
    "        user_based_pred = user_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val[mask_rated_j]) + EPSILON)\n",
    "        user_based_pred = np.clip(user_based_pred, 0, 5)\n",
    "\n",
    "        test_ds_pred[i][j] = user_based_pred\n",
    "        \n",
    "test_ds_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUTn4kSFb8ZA"
   },
   "source": [
    "## Compute MAE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JCVmexyb8ZA",
    "outputId": "6f650170-e9a6-482a-b695-5fa474d964b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7657677902122191, RMSE: 0.9744411877844816\n"
     ]
    }
   ],
   "source": [
    "# MAE\n",
    "MAE = np.sum(np.abs(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1))\n",
    "\n",
    "# RMSE\n",
    "RMSE = np.sqrt(np.sum(np.square(test_ds_pred - test_ds.values)) / np.sum(np.clip(test_ds.values, 0, 1)))\n",
    "\n",
    "print(\"MAE: {}, RMSE: {}\" .format(MAE, RMSE))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3_framework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
